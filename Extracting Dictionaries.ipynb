{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Dictionaries\n",
    "\n",
    "This script extracts dictionaries from a spreadsheet for use in concept analysis. It uses the General Inquirer speadsheet \"inquirerbasic.csv\" which has classes of words. It will extract a dictionary (list of words) and save it to a text file for use in dictionary based analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking files\n",
    "\n",
    "Here we check where our CSV file is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic CSV Handling.ipynb             Truths.Concordance.txt\r\n",
      "Concordances.ipynb                   Untitled.ipynb\r\n",
      "Counting Word Types.ipynb            Untitled1.ipynb\r\n",
      "ExampleTable.csv                     Untitled2.ipynb\r\n",
      "Extracting Dictionaries.ipynb        Untitled3.ipynb\r\n",
      "Hume Enquiry.txt                     Using TextBlob.ipynb\r\n",
      "Python language notes.ipynb          Web Scraping.ipynb\r\n",
      "ScrapeResults.txt                    Web Scraping.ipynb 2\r\n",
      "ScrapeResults.xml                    \u001b[31mcountdocsmatrix.ipynb\u001b[m\u001b[m*\r\n",
      "Sentiment Over a Text.ipynb          fortmactweets.may3-4.2016.txt\r\n",
      "Teaching IPython to Humanists.ipynb  inquirerbasic.csv\r\n",
      "Truth.Concordance.txt                theText.txt\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data out of the CSV\n",
    "\n",
    "Here we open the named CSV, extract the data and then display the first row of headings that shows us the possible categories. The idea is that you can then pick the category that you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csvFile = \"inquirerbasic.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 :  Positiv\n",
      "3 :  Negativ\n",
      "4 :  Pstv\n",
      "5 :  Affil\n",
      "6 :  Ngtv\n",
      "7 :  Hostile\n",
      "8 :  Strong\n",
      "9 :  Power\n",
      "10 :  Weak\n",
      "11 :  Submit\n",
      "12 :  Active\n",
      "13 :  Passive\n",
      "14 :  Pleasur\n",
      "15 :  Pain\n",
      "16 :  Feel\n",
      "17 :  Arousal\n",
      "18 :  EMOT\n",
      "19 :  Virtue\n",
      "20 :  Vice\n",
      "21 :  Ovrst\n",
      "22 :  Undrst\n",
      "23 :  Academ\n",
      "24 :  Doctrin\n",
      "25 :  Econ@\n",
      "26 :  Exch\n",
      "27 :  ECON\n",
      "28 :  Exprsv\n",
      "29 :  Legal\n",
      "30 :  Milit\n",
      "31 :  Polit@\n",
      "32 :  POLIT\n",
      "33 :  Relig\n",
      "34 :  Role\n",
      "35 :  COLL\n",
      "36 :  Work\n",
      "37 :  Ritual\n",
      "38 :  SocRel\n",
      "39 :  Race\n",
      "40 :  Kin@\n",
      "41 :  MALE\n",
      "42 :  Female\n",
      "43 :  Nonadlt\n",
      "44 :  HU\n",
      "45 :  ANI\n",
      "46 :  PLACE\n",
      "47 :  Social\n",
      "48 :  Region\n",
      "49 :  Route\n",
      "50 :  Aquatic\n",
      "51 :  Land\n",
      "52 :  Sky\n",
      "53 :  Object\n",
      "54 :  Tool\n",
      "55 :  Food\n",
      "56 :  Vehicle\n",
      "57 :  BldgPt\n",
      "58 :  ComnObj\n",
      "59 :  NatObj\n",
      "60 :  BodyPt\n",
      "61 :  ComForm\n",
      "62 :  COM\n",
      "63 :  Say\n",
      "64 :  Need\n",
      "65 :  Goal\n",
      "66 :  Try\n",
      "67 :  Means\n",
      "68 :  Persist\n",
      "69 :  Complet\n",
      "70 :  Fail\n",
      "71 :  NatrPro\n",
      "72 :  Begin\n",
      "73 :  Vary\n",
      "74 :  Increas\n",
      "75 :  Decreas\n",
      "76 :  Finish\n",
      "77 :  Stay\n",
      "78 :  Rise\n",
      "79 :  Exert\n",
      "80 :  Fetch\n",
      "81 :  Travel\n",
      "82 :  Fall\n",
      "83 :  Think\n",
      "84 :  Know\n",
      "85 :  Causal\n",
      "86 :  Ought\n",
      "87 :  Perceiv\n",
      "88 :  Compare\n",
      "89 :  Eval@\n",
      "90 :  EVAL\n",
      "91 :  Solve\n",
      "92 :  Abs@\n",
      "93 :  ABS\n",
      "94 :  Quality\n",
      "95 :  Quan\n",
      "96 :  NUMB\n",
      "97 :  ORD\n",
      "98 :  CARD\n",
      "99 :  FREQ\n",
      "100 :  DIST\n",
      "101 :  Time@\n",
      "102 :  TIME\n",
      "103 :  Space\n",
      "104 :  POS\n",
      "105 :  DIM\n",
      "106 :  Rel\n",
      "107 :  COLOR\n",
      "108 :  Self\n",
      "109 :  Our\n",
      "110 :  You\n",
      "111 :  Name\n",
      "112 :  Yes\n",
      "113 :  No\n",
      "114 :  Negate\n",
      "115 :  Intrj\n",
      "116 :  IAV\n",
      "117 :  DAV\n",
      "118 :  SV\n",
      "119 :  IPadj\n",
      "120 :  IndAdj\n",
      "121 :  PowGain\n",
      "122 :  PowLoss\n",
      "123 :  PowEnds\n",
      "124 :  PowAren\n",
      "125 :  PowCon\n",
      "126 :  PowCoop\n",
      "127 :  PowAuPt\n",
      "128 :  PowPt\n",
      "129 :  PowDoct\n",
      "130 :  PowAuth\n",
      "131 :  PowOth\n",
      "132 :  PowTot\n",
      "133 :  RcEthic\n",
      "134 :  RcRelig\n",
      "135 :  RcGain\n",
      "136 :  RcLoss\n",
      "137 :  RcEnds\n",
      "138 :  RcTot\n",
      "139 :  RspGain\n",
      "140 :  RspLoss\n",
      "141 :  RspOth\n",
      "142 :  RspTot\n",
      "143 :  AffGain\n",
      "144 :  AffLoss\n",
      "145 :  AffPt\n",
      "146 :  AffOth\n",
      "147 :  AffTot\n",
      "148 :  WltPt\n",
      "149 :  WltTran\n",
      "150 :  WltOth\n",
      "151 :  WltTot\n",
      "152 :  WlbGain\n",
      "153 :  WlbLoss\n",
      "154 :  WlbPhys\n",
      "155 :  WlbPsyc\n",
      "156 :  WlbPt\n",
      "157 :  WlbTot\n",
      "158 :  EnlGain\n",
      "159 :  EnlLoss\n",
      "160 :  EnlEnds\n",
      "161 :  EnlPt\n",
      "162 :  EnlOth\n",
      "163 :  EnlTot\n",
      "164 :  SklAsth\n",
      "165 :  SklPt\n",
      "166 :  SklOth\n",
      "167 :  SklTot\n",
      "168 :  TrnGain\n",
      "169 :  TrnLoss\n",
      "170 :  TranLw\n",
      "171 :  MeansLw\n",
      "172 :  EndsLw\n",
      "173 :  ArenaLw\n",
      "174 :  PtLw\n",
      "175 :  Nation\n",
      "176 :  Anomie\n",
      "177 :  NegAff\n",
      "178 :  PosAff\n",
      "179 :  SureLw\n",
      "180 :  If\n",
      "181 :  NotLw\n",
      "182 :  TimeSpc\n",
      "183 :  FormLw\n",
      "184 :  Othtags\n",
      "185 :  Defined\n"
     ]
    }
   ],
   "source": [
    "listOfRows = []\n",
    "with open(csvFile, 'r') as file: # This makes sure that file is closed after reading\n",
    "    data = csv.reader(file)\n",
    "    for row in data:\n",
    "        listOfRows.append(row)\n",
    "file.closed\n",
    "\n",
    "categories = listOfRows[0]\n",
    "for i in range(2,len(categories)): # Note that we start at 2 as the first to labels are not categories\n",
    "    print(str(i), \": \", categories[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract category words\n",
    "\n",
    "Here we take the category column number you want (corresponding to the category you want) and extract all the words that match. We then give you the number of words and the first 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1915 words in category: Positiv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ABIDE',\n",
       " 'ABILITY',\n",
       " 'ABLE',\n",
       " 'ABOUND',\n",
       " 'ABSOLVE',\n",
       " 'ABSORBENT',\n",
       " 'ABSORPTION',\n",
       " 'ABUNDANCE',\n",
       " 'ABUNDANT',\n",
       " 'ACCEDE',\n",
       " 'ACCENTUATE',\n",
       " 'ACCEPT',\n",
       " 'ACCEPTABLE',\n",
       " 'ACCEPTANCE',\n",
       " 'ACCESSIBLE',\n",
       " 'ACCESSION',\n",
       " 'ACCLAIM',\n",
       " 'ACCLAMATION',\n",
       " 'ACCOLADE',\n",
       " 'ACCOMMODATE',\n",
       " 'ACCOMMODATION',\n",
       " 'ACCOMPANIMENT',\n",
       " 'ACCOMPLISH',\n",
       " 'ACCOMPLISHMENT',\n",
       " 'ACCORD#2',\n",
       " 'ACCORD#3',\n",
       " 'ACCORD#5',\n",
       " 'ACCORDANCE',\n",
       " 'ACCOUNTABLE',\n",
       " 'ACCRUE',\n",
       " 'ACCURACY',\n",
       " 'ACCURATE',\n",
       " 'ACCURATENESS',\n",
       " 'ACHIEVE',\n",
       " 'ACHIEVEMENT',\n",
       " 'ACKNOWLEDGEMENT',\n",
       " 'ACQUAINT',\n",
       " 'ACQUAINTANCE',\n",
       " 'ACQUIT',\n",
       " 'ACQUITTAL',\n",
       " 'ACTUAL#1',\n",
       " 'ACTUAL#2',\n",
       " 'ACTUALITY',\n",
       " 'ADAMANT',\n",
       " 'ADAPTABILITY',\n",
       " 'ADAPTABLE',\n",
       " 'ADAPTATION',\n",
       " 'ADAPTIVE',\n",
       " 'ADEPT',\n",
       " 'ADEPTNESS']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category = 2\n",
    "\n",
    "words = []\n",
    "for row in listOfRows[1:]: # We iterate over the rows skipping the header row\n",
    "    if row[category] != \"\":\n",
    "        words.append(row[0])\n",
    "\n",
    "print(str(len(words)) + \" words in category:\" + \" \" + categories[category])\n",
    "words[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the list\n",
    "\n",
    "Now we clean the list of the repeating words (which have more than one sense.) Note that we just keep one copy of the word. We print out the new number of words in the category and the first 50 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1631 words in cleaned category: Positiv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ABIDE',\n",
       " 'ABILITY',\n",
       " 'ABLE',\n",
       " 'ABOUND',\n",
       " 'ABSOLVE',\n",
       " 'ABSORBENT',\n",
       " 'ABSORPTION',\n",
       " 'ABUNDANCE',\n",
       " 'ABUNDANT',\n",
       " 'ACCEDE',\n",
       " 'ACCENTUATE',\n",
       " 'ACCEPT',\n",
       " 'ACCEPTABLE',\n",
       " 'ACCEPTANCE',\n",
       " 'ACCESSIBLE',\n",
       " 'ACCESSION',\n",
       " 'ACCLAIM',\n",
       " 'ACCLAMATION',\n",
       " 'ACCOLADE',\n",
       " 'ACCOMMODATE',\n",
       " 'ACCOMMODATION',\n",
       " 'ACCOMPANIMENT',\n",
       " 'ACCOMPLISH',\n",
       " 'ACCOMPLISHMENT',\n",
       " 'ACCORD',\n",
       " 'ACCORDANCE',\n",
       " 'ACCOUNTABLE',\n",
       " 'ACCRUE',\n",
       " 'ACCURACY',\n",
       " 'ACCURATE',\n",
       " 'ACCURATENESS',\n",
       " 'ACHIEVE',\n",
       " 'ACHIEVEMENT',\n",
       " 'ACKNOWLEDGEMENT',\n",
       " 'ACQUAINT',\n",
       " 'ACQUAINTANCE',\n",
       " 'ACQUIT',\n",
       " 'ACQUITTAL',\n",
       " 'ACTUAL',\n",
       " 'ACTUALITY',\n",
       " 'ADAMANT',\n",
       " 'ADAPTABILITY',\n",
       " 'ADAPTABLE',\n",
       " 'ADAPTATION',\n",
       " 'ADAPTIVE',\n",
       " 'ADEPT',\n",
       " 'ADEPTNESS',\n",
       " 'ADEQUATE',\n",
       " 'ADHERENCE',\n",
       " 'ADHERENT']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanedWords = []\n",
    "theLastWord = \"\"\n",
    "for word in words:\n",
    "    if \"#\" in word:\n",
    "        if theLastWord not in word:\n",
    "            cleanedWords.append(word[:-2])\n",
    "            theLastWord = (word[:-2])\n",
    "    else:\n",
    "        cleanedWords.append(word)\n",
    "        theLastWord = word\n",
    "\n",
    "print(str(len(cleanedWords)) + \" words in cleaned category:\" + \" \" + categories[category])\n",
    "cleanedWords[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the list of words\n",
    "\n",
    "Finally, we save the list of words. Note that we lowercase the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "nameOfDict = categories[category] + \".dictionary.txt\"\n",
    "\n",
    "with open(nameOfDict, \"w\") as fileToWrite:\n",
    "    for word in cleanedWords:\n",
    "        fileToWrite.write(word.lower() + \"\\n\")\n",
    "    \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "You can now use this list with a dictionary based sentiment analysis tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
