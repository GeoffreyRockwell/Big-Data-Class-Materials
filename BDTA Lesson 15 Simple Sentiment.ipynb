{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BDTA Lesson 15: Simple Sentiment Analysis Example\n",
    "\n",
    "This notebook shows how dictionary based sentiment analysis can work. It is based on Neal Caron's [An introduction to text analysis with Python, Part 1](http://nealcaren.web.unc.edu/an-introduction-to-text-analysis-with-python-part-1/).\n",
    "\n",
    "The program shows how to analyze a couple of sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up our data\n",
    "\n",
    "Here we will define the data to test and our positive and negative dictionaries. The data is put into variables with different data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theText = \"No food is good food. Ha. I'm on a diet and the food is awful and lame.\"\n",
    "positiveWords=['awesome','good','nice','super','fun','delightful']\n",
    "negativeWords=['awful','lame','horrible','bad']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing on sentences\n",
    "\n",
    "Now we will divide the text into sentences so we can get the sentiment of each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function sent_tokenize in module nltk.tokenize:\n",
      "\n",
      "sent_tokenize(text, language='english')\n",
      "    Return a sentence-tokenized copy of *text*,\n",
      "    using NLTK's recommended sentence tokenizer\n",
      "    (currently :class:`.PunktSentenceTokenizer`\n",
      "    for the specified language).\n",
      "    \n",
      "    :param text: text to split into sentences\n",
      "    :param language: the model name in the Punkt corpus\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sent_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This function takes a text and tokenizes it into a list of sentences\n",
    "def tokenSentences(textIn):\n",
    "    theSentences = sent_tokenize(textIn) # Remember to use local variable\n",
    "    return theSentences\n",
    "\n",
    "# Test how many sentences we get\n",
    "len(tokenSentences(theText))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the text\n",
    "\n",
    "Now we will create a function for tokenizing the sentences and test it. This uses a module **re** (Regular Expression Operations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no', 'food', 'is', 'good', 'food']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def tokenizer(txt2Token): # Function for tokenizing\n",
    "    theTokens = re.findall(r'\\b\\w[\\w-]*\\b', txt2Token.lower())\n",
    "    return theTokens\n",
    "    \n",
    "# We test if the function works with the first sentnece from the list above\n",
    "tokensOfSent = tokenizer(tokenSentences(theText)[0])\n",
    "print(tokensOfSent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating associations of words\n",
    "\n",
    "Now we will create funciton that counts the number of positive or negative words. The idea is that we pass the function a list of tokens (of the sentence) and a list of words that have emotion. It the counts how many emotion words are in the list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ['good']\n"
     ]
    }
   ],
   "source": [
    "# Function that counts how many target words are in a list of tokens\n",
    "def countSentimentalTokens(listOfTokens,listOfTargetWords):\n",
    "    numTargetWords = 0\n",
    "    matchedWords = []\n",
    "    for token in listOfTokens: # Goes through the tokens in the list\n",
    "        if token in listOfTargetWords: # For each one it checks if it is in the target list\n",
    "            numTargetWords += 1\n",
    "            matchedWords.append(token)\n",
    "    return numTargetWords, matchedWords # Note that we are returning a tuple (2 values)\n",
    "\n",
    "theTuple = countSentimentalTokens(tokensOfSent,positiveWords)\n",
    "print(str(theTuple[0]) + \" \" + str(theTuple[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating percentage\n",
    "\n",
    "Now we can calculate the percentages of postive and negative words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: 20%  Negative: 0%\n"
     ]
    }
   ],
   "source": [
    "def calculatePercent(listOfTokens,positiveList,negativeList):\n",
    "    numWords = len(listOfTokens) # How many words total\n",
    "    \n",
    "    # We call the function to count the tokens from the positive list in the sentence\n",
    "    positiveMatches = countSentimentalTokens(listOfTokens,positiveList) \n",
    "    percntPos = positiveMatches[0] / numWords # We divide by the total number of words for percentage\n",
    "    \n",
    "    # We call the function to count the tokens from the negative list in the sentence\n",
    "    negativeMatches = countSentimentalTokens(listOfTokens,negativeList)\n",
    "    percntNeg = negativeMatches[0] / numWords # We divide by the total number of words for percentage\n",
    "\n",
    "    return percntPos, percntNeg # We return the percentage of positive and negative words\n",
    "\n",
    "# We test the function on the first sentence\n",
    "results = calculatePercent(tokensOfSent,positiveWords,negativeWords)\n",
    "print(\"Positive: \" + \"{:.0%}\".format(results[0]) + \"  Negative: \" + \"{:.0%}\".format(results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate sentiment\n",
    "\n",
    "Here we calculate whether a sentence is positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculateSentiment(percntPos,percntNeg):\n",
    "    sentiment = percntPos - percntNeg # Subtract the percentage of negative words from positive words\n",
    "    return sentiment\n",
    "\n",
    "# Test what we get\n",
    "calculateSentiment(results[0],results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process sentences\n",
    "\n",
    "Finally, we have a function that can process a text. It first tokenizes the text into sentences using the function above. It then processes each sentence:\n",
    "* It tokenizes the sentence\n",
    "* It calls the percentages of positive and negative words calling the function above\n",
    "* It calculates if the sentiment is positive or negative\n",
    "* It returns a list of the sentiment calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2, 0.0, -0.16666666666666666]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def processText(textIn,posMatchWords,negMatchWords):\n",
    "    listOfSentences = tokenSentences(textIn) # Tokenize the text\n",
    "    \n",
    "    listOfSentiments = []\n",
    "    for sentence in listOfSentences: # Process sentence by sentence\n",
    "        sentTokens = tokenizer(sentence) # Tokenize the sentences\n",
    "        percentages = calculatePercent(sentTokens,posMatchWords,negMatchWords) # Calculates percents\n",
    "        theSentiment = calculateSentiment(percentages[0],percentages[1]) # Calculates sentiment\n",
    "        listOfSentiments.append(theSentiment) # Appends sentiment to list\n",
    "        \n",
    "    return listOfSentiments # Return the final list\n",
    "\n",
    "# Test the function\n",
    "theFinalList = processText(theText,positiveWords,negativeWords)\n",
    "theFinalList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Exercise\n",
    "\n",
    "Plot the list of sentiment calculations to see if there is a trend.\n",
    "\n",
    "**Optional**: \n",
    "* Can you edit the appropriate functions to return a list of all the positive words used? Note that the function '''countSentimentalTokens()''' already returns a list of the words used. What else do you need to add to?\n",
    "* Can you open a text file and process it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Homework\n",
    "\n",
    "Create a notebook that can calculate the sentiment of tweets using dictionaries of positive and negative words. Instead of getting the data from within the notebook, use a prepared list of positive and negative words and a list of tweets. Here are the links:\n",
    "\n",
    "- Positive data http://www.unc.edu/~ncaren/haphazard/positive.txt\n",
    "- Negative data http://www.unc.edu/~ncaren/haphazard/negative.txt\n",
    "- Obama tweets http://www.unc.edu/~ncaren/haphazard/obama_tweets.txt\n",
    "\n",
    "You should download these and then add programming to open the textfiles and split on the new lines (\\\\n). \n",
    "\n",
    "    '''posTokens = posText.split(\"\\n\")'''\n",
    "\n",
    "This will give you a list of positive tokens or negative tokens or a list of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
